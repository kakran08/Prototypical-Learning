{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Prototypical Learning**"
      ],
      "metadata": {
        "id": "V6cXY4rDe0yR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcF7Gr3JhBQK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms,datasets, models\n",
        "import os\n",
        "import random\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define neural network for protypical learning\n",
        "class ProtypicalNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ProtypicalNetwork, self).__init__()\n",
        "        self.vgg16 = models.vgg16(pretrained=True)\n",
        "        self.feature_extractor = nn.Sequential(*list(self.vgg16.features.children()))\n",
        "\n",
        "        for param in self.feature_extractor.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.fc1 = nn.Linear(512, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = torch.relu(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "n9Xn7WgFhqnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate distance between samples\n",
        "def euclidean_distance(x1, x2):\n",
        "    return torch.norm(x1[:,None]-x2, dim=2)"
      ],
      "metadata": {
        "id": "Jb3n-Z5SjIiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Omniglot data from internet\n",
        "def get_omniglot_data(path='./data'):\n",
        "    # Create directory path if not present\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    # Define the transform to apply to the images\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(), # Convert PIL image to PyTorch tensor\n",
        "        transforms.Resize((32, 32)), # Resize to 32x32 for consistency\n",
        "        transforms.Normalize((0.5,), (0.5,)) # Normalize pixel values\n",
        "    ])\n",
        "    # Download and load the Omniglot dataset (background set)\n",
        "    train_dataset = datasets.Omniglot(\n",
        "        root=path,\n",
        "        background=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "    # Download and load the Omniglot dataset (evaluation set)\n",
        "    # test_dataset = datasets.Omniglot(\n",
        "    #     root=path,\n",
        "    #     background=False,\n",
        "    #     download=True,\n",
        "    #     transform=transform\n",
        "    # )\n",
        "    return train_dataset, test_dataset\n",
        "get_omniglot_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkMqu0_dmDhP",
        "outputId": "bd289046-2a60-4d95-d1d4-205d8b8a9c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.46M/9.46M [00:00<00:00, 354MB/s]\n",
            "100%|██████████| 6.46M/6.46M [00:00<00:00, 360MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset Omniglot\n",
              "     Number of datapoints: 19280\n",
              "     Root location: ./data/omniglot-py\n",
              "     StandardTransform\n",
              " Transform: Compose(\n",
              "                ToTensor()\n",
              "                Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)\n",
              "                Normalize(mean=(0.5,), std=(0.5,))\n",
              "            ),\n",
              " Dataset Omniglot\n",
              "     Number of datapoints: 13180\n",
              "     Root location: ./data/omniglot-py\n",
              "     StandardTransform\n",
              " Transform: Compose(\n",
              "                ToTensor()\n",
              "                Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)\n",
              "                Normalize(mean=(0.5,), std=(0.5,))\n",
              "            ))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data for train step\n",
        "def load_data(path, num_classes = 1, train_samples = 5, test_samples = 3):\n",
        "  # Get list of class directories\n",
        "  class_dirs = []\n",
        "  for lang in os.listdir(path):\n",
        "    if os.path.isdir(os.path.join(path, lang)):\n",
        "      for char in os.listdir(os.path.join(path, lang)):\n",
        "        if os.path.isdir(os.path.join(path, lang, char)):\n",
        "          class_dirs.append(os.path.join(path, lang, char))\n",
        "  random.shuffle(class_dirs) # get list of randomized class directories\n",
        "  class_dirs = class_dirs[:num_classes] # get first num_classes\n",
        "  # Variables for support set and query set\n",
        "  support_set = []\n",
        "  query_set = []\n",
        "  support_labels = []\n",
        "  query_labels = []\n",
        "  for cls_dir in class_dirs:\n",
        "    # Get 'language_character{num}' as class label\n",
        "    cls_name = \"_\".join(cls_dir.split(os.sep)[-2:])\n",
        "    images = [os.path.join(cls_dir, img) for img in os.listdir(cls_dir) if img.endswith('.png')]\n",
        "    # Check for availablity of images for support set and query set\n",
        "    if len(images) >= train_samples + test_samples:\n",
        "      sampled_images = random.sample(images, train_samples + test_samples)\n",
        "      support_set.extend(sampled_images[:-test_samples])\n",
        "      query_set.extend(sampled_images[-test_samples:train_samples+test_samples])\n",
        "      support_labels.append([cls_name]*train_samples)\n",
        "      query_labels.append([cls_name]*test_samples)\n",
        "      # Stop if support set have num_classes*train_samples\n",
        "      if len(support_set) >= num_classes*train_samples:\n",
        "        break\n",
        "  return support_set, query_set, support_labels, query_labels\n",
        "\n",
        "\n",
        "# s, q, l_s, l_q = load_data(path='./data/omniglot-py/images_background')"
      ],
      "metadata": {
        "id": "U5dxdJZwoZ42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train Protypical Network\n",
        "def train(model, optimizer, epochs=10, num_classes=1, train_samples=5, test_samples=3):\n",
        "    history = {}\n",
        "    correct_predictions = 0\n",
        "    total_queries = 0\n",
        "    epoch = 0\n",
        "    # Define the transform to apply to the images\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(), # Convert PIL image to PyTorch tensor\n",
        "        transforms.Resize((32, 32)), # Resize to 32x32 for consistency\n",
        "        transforms.Normalize((0.5,), (0.5,)) # Normalize pixel values\n",
        "    ])\n",
        "    model.cuda()\n",
        "    while epoch < epochs:\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        # load support set and query set\n",
        "        support_set, query_set, support_labels, query_labels = load_data(path='./data/omniglot-py/images_background', num_classes=num_classes, train_samples=train_samples, test_samples=test_samples)\n",
        "        if not support_set or not query_set:\n",
        "          continue\n",
        "        else:\n",
        "          epoch += 1\n",
        "          print(f\"Epoch {epoch}/{epochs}:\")\n",
        "          history[epoch] = {}\n",
        "          history[\"all_losses\"] = float()\n",
        "          history[\"predictions\"] = []\n",
        "          history[\"labels\"] = []\n",
        "\n",
        "        print(f\"Loading support set images: {len(support_set)} Images\")\n",
        "        support_set = torch.stack([transform((Image.open(img)).convert('RGB')) for img in support_set]).cuda()\n",
        "\n",
        "        print(f\"Loading query set images: {len(query_set)} Images\")\n",
        "        query_set = torch.stack([transform((Image.open(img)).convert('RGB')) for img in query_set]).cuda()\n",
        "\n",
        "        # Generate predictions\n",
        "        support_preds = model(support_set)\n",
        "        query_preds = model(query_set)\n",
        "\n",
        "        # Prototypes for each class\n",
        "        prototypes = support_preds.view(-1, train_samples, 64).mean(dim=1)\n",
        "\n",
        "        # Calculate distance between prototypes and query outputs\n",
        "        distances = euclidean_distance(query_preds, prototypes) # Shape: (num_queries, num_classes)\n",
        "\n",
        "        # Create labels for query_set\n",
        "        query_labels = []\n",
        "        for i in range(num_classes):\n",
        "          query_labels.extend([i]*test_samples)\n",
        "        query_labels = torch.tensor(query_labels).cuda()\n",
        "\n",
        "        # Calculate log probabilities\n",
        "        log_probs = -distances\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = nn.CrossEntropyLoss()(log_probs, query_labels)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        history[epoch][\"loss\"] = loss.item()\n",
        "        history[\"all_losses\"] += loss.item()\n",
        "\n",
        "        # Performance measures\n",
        "        _, predicted = torch.min(distances, dim=1)\n",
        "        correct_predictions += (predicted == query_labels).sum().item()\n",
        "        total_queries += len(query_labels)\n",
        "        accuracy = correct_predictions / total_queries\n",
        "\n",
        "        history[epoch][\"accuracy\"] = accuracy\n",
        "        history[\"predictions\"].extend(predicted.cpu().numpy())\n",
        "        history[\"labels\"].extend(query_labels.cpu().numpy())\n",
        "\n",
        "        print(f\"Epoch {epoch}: Loss = {loss.item()}, Accuracy = {accuracy}\")\n",
        "    print(f\"Final accuracy: {correct_predictions/total_queries}\")\n",
        "    return history, prototypes\n",
        "\n",
        "model = ProtypicalNetwork(input_dim=3, hidden_dim=64, output_dim=64)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "history, proto = train(model=model, optimizer=optimizer, epochs=10, num_classes=75, train_samples=15, test_samples=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRoir9UrjpgA",
        "outputId": "d0c2d772-fec9-4032-ded1-a56509b70855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:03<00:00, 178MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10:\n",
            "Loading support set images: 1125 Images\n",
            "Loading query set images: 225 Images\n",
            "Epoch 1: Loss = 3.96459698677063, Accuracy = 0.5288888888888889\n",
            "Epoch 2/10:\n",
            "Loading support set images: 1125 Images\n",
            "Loading query set images: 225 Images\n",
            "Epoch 2: Loss = 3.860280990600586, Accuracy = 0.5333333333333333\n",
            "Epoch 3/10:\n",
            "Loading support set images: 1125 Images\n",
            "Loading query set images: 225 Images\n",
            "Epoch 3: Loss = 3.733813762664795, Accuracy = 0.5318518518518518\n",
            "Epoch 4/10:\n",
            "Loading support set images: 1125 Images\n",
            "Loading query set images: 225 Images\n",
            "Epoch 4: Loss = 3.648183822631836, Accuracy = 0.5366666666666666\n",
            "Epoch 5/10:\n",
            "Loading support set images: 1125 Images\n",
            "Loading query set images: 225 Images\n",
            "Epoch 5: Loss = 3.59014630317688, Accuracy = 0.5306666666666666\n",
            "Epoch 6/10:\n",
            "Loading support set images: 1125 Images\n",
            "Loading query set images: 225 Images\n",
            "Epoch 6: Loss = 3.4132184982299805, Accuracy = 0.5259259259259259\n",
            "Epoch 7/10:\n",
            "Loading support set images: 1125 Images\n",
            "Loading query set images: 225 Images\n",
            "Epoch 7: Loss = 3.3254919052124023, Accuracy = 0.5219047619047619\n",
            "Epoch 8/10:\n",
            "Loading support set images: 1125 Images\n",
            "Loading query set images: 225 Images\n",
            "Epoch 8: Loss = 3.2296957969665527, Accuracy = 0.5183333333333333\n",
            "Epoch 9/10:\n",
            "Loading support set images: 1125 Images\n",
            "Loading query set images: 225 Images\n",
            "Epoch 9: Loss = 3.2370080947875977, Accuracy = 0.5061728395061729\n",
            "Epoch 10/10:\n",
            "Loading support set images: 1125 Images\n",
            "Loading query set images: 225 Images\n",
            "Epoch 10: Loss = 3.0863709449768066, Accuracy = 0.5022222222222222\n",
            "Final accuracy: 0.5022222222222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "proto.shape"
      ],
      "metadata": {
        "id": "Zw1sWqCXAQzM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ae9cda7-a109-4b3c-b2ee-ee6f712be81a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([75, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vc-m79GY4kus"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}